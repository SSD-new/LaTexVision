# LaTexVision

**LaTexVision** — это мощный инструмент на базе Искусственного Интеллекта для конвертации изображений и PDF-документов с рукописным или печатным математическим текстом в чистый, отформатированный LaTeX код.

Проект поддерживает гибридный режим работы:
1.  **Cloud Mode:** Быстрая работа через Google Gemini API (требует интернет).
2.  **Local Mode:** Полностью автономная работа через локальную нейросеть Qwen2.5-VL (требует GPU, работает без интернета).

---

## Возможности

*   **Поддержка PDF и изображений:** Обработка многостраничных документов.
*   **Умная сегментация:** Использование OpenCV для автоматического определения блоков текста и формул.
*   **Ручное редактирование:** Инструменты для удаления артефактов (ластик), разделения колонок и разрезания строк.
*   **Live Preview:** Встроенный редактор кода с мгновенным рендерингом LaTeX (KaTeX).
*   **Приватность:** Возможность запускать LLM локально на своем оборудовании.
*   **AI Рефакторинг:** Возможность попросить ИИ исправить ошибки или изменить стиль уже распознанного текста.

---

## Системные требования

### Для Фронтенда (Frontend)
*   **Node.js:** Версия 18.0 или выше.
*   **Менеджер пакетов:** npm, yarn или pnpm.

### Для Локального Бэкенда (Backend)
*   **OS:** Windows / Linux (с поддержкой CUDA).
*   **Python:** 3.10+.
*   **GPU:** NVIDIA GPU с поддержкой CUDA (рекомендуется от 8GB VRAM для 4-битной квантованной модели, 12GB+ для полной производительности).
*   **RAM:** 16GB+.

---

## Установка и Запуск

### 1. Фронтенд (Интерфейс)

Клиентская часть написана на React + Vite.

1.  **Клонируйте репозиторий:**
    ```bash
    git clone https://github.com/your-repo/latex-vision.git
    cd latex-vision
    ```

2.  **Установите зависимости:**
    ```bash
    npm install
    ```

3.  **Настройка API ключа (для Cloud режима):**
    Создайте файл `.env.local` в корне проекта и добавьте ваш ключ от Google Gemini:
    ```env
    API_KEY=ваш_ключ_от_google_ai_studio
    ```
    *Если вы планируете использовать только локальный сервер, этот шаг можно пропустить.*

4.  **Запуск в режиме разработки:**
    ```bash
    npm run dev
    ```
    Приложение будет доступно по адресу: `http://localhost:3000`

---

### 2. Бэкенд (Локальный сервер Python)

Сервер необходим, если вы хотите работать **без интернета** или использовать модель Qwen2.5-VL локально.

1.  **Перейдите в папку backend:**
    *(Если папки нет, создайте её и поместите туда файлы `server.txt` (переименуйте в `server.py`) и `requirements.txt`)*.

2.  **Создайте виртуальное окружение (рекомендуется):**
    ```bash
    # Windows
    python -m venv venv
    venv\Scripts\activate

    # Linux / macOS
    python3 -m venv venv
    source venv/bin/activate
    ```

3.  **Установите PyTorch с поддержкой CUDA:**
    Посетите [pytorch.org](https://pytorch.org/get-started/locally/) для получения команды под вашу систему. Обычно это выглядит так:
    ```bash
    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
    ```

4.  **Установите остальные зависимости:**
    ```bash
    pip install flask flask-cors Pillow transformers accelerate qwen-vl-utils bitsandbytes
    ```

5.  **Загрузка модели (Qwen2.5-VL-7B-Instruct):**
    Вам необходимо скачать веса модели с Hugging Face. Вы можете сделать это через Python-скрипт или `huggingface-cli`.

    Пример скрипта для скачивания (`download_model.py`):
    ```python
    from huggingface_hub import snapshot_download
    snapshot_download(repo_id="Qwen/Qwen2.5-VL-7B-Instruct", local_dir="D:/models/Qwen2.5-VL-7B-Instruct")
    ```
    *Примечание: Модель весит около 15 ГБ.*

6.  **Настройка пути к модели:**
    Откройте файл `backend/server.py` и найдите строку:
    ```python
    MODEL_PATH = r"D:\models\Qwen2.5-VL-7B-Instruct"
    ```
    Замените путь на тот, куда вы скачали модель.

7.  **Запуск сервера:**
    ```bash
    python backend/server.py
    ```
    Сервер запустится на порту `5000`.

---

## Использование

1.  Откройте веб-приложение (`http://localhost:3000`).
2.  **Для работы через Облако (Gemini):**
    *   Просто загрузите изображение. Убедитесь, что интернет включен.
3.  **Для работы Локально (Offline):**
    *   Нажмите иконку **Настройки** (шестеренка) в шапке сайта.
    *   Включите переключатель **"Локальный сервер (Offline)"**.
    *   Убедитесь, что адрес сервера указан верно (по умолчанию `http://localhost:5000`).
    *   Сохраните настройки.
4.  Загрузите изображение или PDF.
5.  Дождитесь сегментации (появятся синие рамки).
6.  При необходимости используйте инструменты слева (ластик, ножницы) для коррекции разметки.
7.  Нажмите **"Конвертировать"**.

---

## Решение проблем

### Ошибка "Mixed Content" / "Network Error" при локальном режиме
Если фронтенд открыт через HTTPS (например, на Vercel), браузер заблокирует запросы к `http://localhost:5000`.
**Решение:**
1. Открывайте фронтенд локально через `http://localhost:3000`.
2. Либо используйте `ngrok` для создания https туннеля к вашему Python серверу:
   ```bash
   ngrok http 5000
   ```
   И укажите полученный `https` адрес в настройках приложения.

### Ошибка "CUDA out of memory"
Если у вас вылетает ошибка памяти видеокарты:
1. Убедитесь, что в `server.py` включена 4-битная квантование (`load_in_4bit=True`).
2. Уменьшите разрешение входящих изображений в функции `resize_image` (переменная `MAX_PIXELS`).
3. Закройте другие приложения, использующие GPU.

### OpenCV не загружается
Фронтенд использует OpenCV.js через CDN. Если у вас нет интернета при запуске фронтенда, скачайте `opencv.js` локально и измените путь в `index.html` и `services/cvWorkerScript.ts`.

---

## Структура проекта

*   `/src` - Исходный код React приложения.
    *   `/services` - Логика работы с API и OpenCV.
    *   `/components` - UI компоненты (Editor, Renderer).
*   `/backend` - Исходный код Python сервера.
*   `/api` - Serverless функции для Vercel (прокси для Gemini API).

---

## Лицензия

Данный проект распространяется под лицензией MIT.

### Сторонние компоненты

При использовании данного программного обеспечения вы соглашаетесь соблюдать условия лицензий сторонних компонентов:

1.  **Qwen2.5-VL**: Веса модели и исходный код распространяются Alibaba Cloud под лицензией Apache 2.0. Использование модели подразумевает согласие с условиями использования Hugging Face и Alibaba Cloud.
2.  **OpenCV.js**: Библиотека компьютерного зрения распространяется под лицензией Apache 2.0.
3.  **KaTeX**: Библиотека для рендеринга математических формул распространяется под лицензией MIT.
4.  **Google Gemini API**: Использование облачного режима регулируется Условиями использования Google AI Studio.

**Отказ от ответственности:**
Программное обеспечение предоставляется "как есть", без каких-либо гарантий, явных или подразумеваемых, включая, но не ограничиваясь, гарантиями товарной пригодности и пригодности для конкретных целей. Авторы и правообладатели не несут ответственности за любые иски, ущерб или иные обязательства, возникшие в результате использования данного программного обеспечения.
